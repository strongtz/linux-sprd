/*
 * Copyright (C) 2012-2015 Spreadtrum Communications Inc.
 *
 * This program is free software; you can redistribute it and/or
 * modify it under the terms of the GNU General Public License
 * as published by the Free Software Foundation; either version 2
 * of the License, or (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 */

#define pr_fmt(fmt) "sprd_cpu_cooling: " fmt

#include <linux/module.h>
#include <linux/thermal.h>
#include <linux/cpufreq.h>

#if defined(CONFIG_ARM_SPRD_HW_CPUFREQ) || defined(CONFIG_ARM_SPRD_SW_CPUFREQ)
#include <linux/sprd-cpufreq.h>
#endif

#include <linux/err.h>
#include <linux/pm_opp.h>
#include <linux/slab.h>
#include <linux/workqueue.h>
#include <linux/cpu.h>
#include <linux/sprd_cpu_cooling.h>
#include <linux/kthread.h>
#include <linux/sched.h>
#include <uapi/linux/sched/types.h>


#include <trace/events/thermal.h>

static DEFINE_IDR(cpufreq_idr);
static DEFINE_MUTEX(cooling_cpufreq_lock);

static unsigned int cpufreq_dev_count;

static DEFINE_MUTEX(cooling_list_lock);
static LIST_HEAD(cpufreq_dev_list);

/**
 * get_idr - function to get a unique id.
 * @idr: struct idr * handle used to create a id.
 * @id: int * value generated by this function.
 *
 * This function will populate @id with an unique
 * id, using the idr API.
 *
 * Return: 0 on success, an error code on failure.
 */
static int get_idr(struct idr *idr, int *id)
{
	int ret;

	mutex_lock(&cooling_cpufreq_lock);
	ret = idr_alloc(idr, NULL, 0, 0, GFP_KERNEL);
	mutex_unlock(&cooling_cpufreq_lock);
	if (unlikely(ret < 0))
		return ret;
	*id = ret;

	return 0;
}

/**
 * release_idr - function to free the unique id.
 * @idr: struct idr * handle used for creating the id.
 * @id: int value representing the unique id.
 */
static void release_idr(struct idr *idr, int id)
{
	mutex_lock(&cooling_cpufreq_lock);
	idr_remove(idr, id);
	mutex_unlock(&cooling_cpufreq_lock);
}

/* Below code defines functions to be used for cpufreq as cooling device */

/**
 * get_level: Find the level for a particular frequency
 * @cpufreq_dev: cpufreq_dev for which the property is required
 * @freq: Frequency
 *
 * Return: level on success, THERMAL_CSTATE_INVALID on error.
 */
static unsigned long get_level(struct cpufreq_cooling_device *cpufreq_dev,
			       unsigned int freq)
{
	unsigned long level;

	for (level = 0; level <= cpufreq_dev->max_level; level++) {
		if (freq == cpufreq_dev->freq_table[level])
			return level;

		if (freq > cpufreq_dev->freq_table[level])
			break;
	}

	return THERMAL_CSTATE_INVALID;
}

/**
 * cpufreq_cooling_get_level - for a given cpu, return the cooling level.
 * @cpu: cpu for which the level is required
 * @freq: the frequency of interest
 *
 * This function will match the cooling level corresponding to the
 * requested @freq and return it.
 *
 * Return: The matched cooling level on success or THERMAL_CSTATE_INVALID
 * otherwise.
 */
unsigned long cpufreq_cooling_get_level(unsigned int cpu, unsigned int freq)
{
	struct cpufreq_cooling_device *cpufreq_dev;

	mutex_lock(&cooling_list_lock);
	list_for_each_entry(cpufreq_dev, &cpufreq_dev_list, node) {
		if (cpumask_test_cpu(cpu, &cpufreq_dev->allowed_cpus)) {
			unsigned long level = get_level(cpufreq_dev, freq);

			mutex_unlock(&cooling_list_lock);
			return level;
		}
	}
	mutex_unlock(&cooling_list_lock);

	pr_err("%s: cpu:%d not part of any cooling device\n", __func__, cpu);
	return THERMAL_CSTATE_INVALID;
}
EXPORT_SYMBOL_GPL(cpufreq_cooling_get_level);

struct cpufreq_cooling_device *
cpufreq_cooling_get_dev_by_name(const char *name)
{
	struct cpufreq_cooling_device *cpufreq_dev = NULL;
	struct cpufreq_cooling_device *ref = ERR_PTR(-EINVAL);
	unsigned int found = 0;

	if (!name)
		goto exit;

	mutex_lock(&cooling_cpufreq_lock);
	list_for_each_entry(cpufreq_dev, &cpufreq_dev_list, node) {
		if (!strncasecmp(name,
				cpufreq_dev->power_model->type,
					THERMAL_NAME_LENGTH)) {
			found++;
			ref = cpufreq_dev;
		}
	}
	mutex_unlock(&cooling_cpufreq_lock);

	/* nothing has been found, thus an error code for it */
	if (found == 0)
		ref = ERR_PTR(-ENODEV);
	else if (found > 1)
		/* Success only when an unique zone is found */
		ref = ERR_PTR(-EEXIST);

exit:
	return ref;
}
EXPORT_SYMBOL_GPL(cpufreq_cooling_get_dev_by_name);

/**
 * cpufreq_thermal_notifier - notifier callback for cpufreq policy change.
 * @nb:	struct notifier_block * with callback info.
 * @event: value showing cpufreq event for which this function invoked.
 * @data: callback-specific data
 *
 * Callback to hijack the notification on cpufreq policy transition.
 * Every time there is a change in policy, we will intercept and
 * update the cpufreq policy with thermal constraints.
 *
 * Return: 0 (success)
 */
static int cpufreq_thermal_notifier(struct notifier_block *nb,
				    unsigned long event, void *data)
{
	struct cpufreq_policy *policy = data;
	unsigned long clipped_freq;
	struct cpufreq_cooling_device *cpufreq_dev;

	if (event != CPUFREQ_ADJUST)
		return NOTIFY_DONE;

	mutex_lock(&cooling_list_lock);
	list_for_each_entry(cpufreq_dev, &cpufreq_dev_list, node) {
		if (!cpumask_test_cpu(policy->cpu, &cpufreq_dev->allowed_cpus))
			continue;

		/*
		 * policy->max is the maximum allowed frequency defined by user
		 * and clipped_freq is the maximum that thermal constraints
		 * allow.
		 *
		 * If clipped_freq is lower than policy->max, then we need to
		 * readjust policy->max.
		 *
		 * But, if clipped_freq is greater than policy->max, we don't
		 * need to do anything.
		 */
		clipped_freq = cpufreq_dev->clipped_freq;

		if (policy->max > clipped_freq)
			cpufreq_verify_within_limits(policy, 0, clipped_freq);
		break;
	}
	mutex_unlock(&cooling_list_lock);

	return NOTIFY_OK;
}

/**
 * build_dyn_power_table() - create a dynamic power to frequency table
 * @cpufreq_device:	the cpufreq cooling device in which to store the table
 *
 * Build a dynamic power to frequency table for this cpu and store it
 * in @cpufreq_device.  This table will be used in cpu_power_to_freq() and
 * cpu_freq_to_power() to convert between power and frequency
 * efficiently.  Power is stored in mW, frequency in KHz.  The
 * resulting table is in ascending order.
 *
 * Return: 0 on success, -EINVAL if there are no OPPs for any CPUs,
 * -ENOMEM if we run out of memory or -EAGAIN if an OPP was
 * added/enabled while the function was executing.
 */
static int build_dyn_power_table(struct cpufreq_cooling_device *cpufreq_device,
				 u32 cluster)
{
	struct power_table *power_table = NULL;
	struct power_table *l2_power_table = NULL;
	struct dev_pm_opp *opp = NULL;
	struct device *dev = NULL;
	int num_opps = 0, cpu, i, ret = 0;
	unsigned long freq;

	for_each_cpu(cpu, &cpufreq_device->allowed_cpus) {
		dev = get_cpu_device(cpu);
		if (!dev) {
			dev_warn(&cpufreq_device->cool_dev->device,
				 "No cpu device for cpu %d\n", cpu);
			continue;
		}

		num_opps = dev_pm_opp_get_opp_count(dev);
		if (num_opps > 0)
			break;
		else if (num_opps < 0)
			return num_opps;
	}

	if (num_opps == 0)
		return -EINVAL;

	power_table = kcalloc(num_opps, sizeof(*power_table), GFP_KERNEL);
	if (!power_table)
		return -ENOMEM;

	l2_power_table = kcalloc(num_opps, sizeof(*l2_power_table), GFP_KERNEL);
	if (!l2_power_table) {
		ret = -ENOMEM;
		goto free_power_table;
	}

	for (freq = 0, i = 0;
	     opp = dev_pm_opp_find_freq_ceil(dev, &freq), !IS_ERR(opp);
	     freq++, i++) {
		u32 freq_mhz, voltage_mv;
		u64 power = 0, l2_power = 0;

		if (i >= num_opps) {
			ret = -EAGAIN;
			goto free_l2_power_table;
		}

		freq_mhz = freq / 1000000;
		voltage_mv = dev_pm_opp_get_voltage(opp) / 1000;
		dev_pm_opp_put(opp);

		/*
		 * Do the multiplication with MHz and millivolt so as
		 * to not overflow.
		 */
		if (cpufreq_device->power_model->cab->get_core_dyn_power_p != NULL)
			power = cpufreq_device->power_model->cab->
				get_core_dyn_power_p(cluster, freq_mhz, voltage_mv);
		if (cpufreq_device->power_model->cab->get_cluster_dyn_power_p != NULL)
			l2_power = cpufreq_device->power_model->cab->
				get_cluster_dyn_power_p(cluster, freq_mhz, voltage_mv);

		/* frequency is stored in power_table in KHz */
		power_table[i].frequency = freq / 1000;
		l2_power_table[i].frequency = freq / 1000;

		/* power is stored in mW */
		power_table[i].power = power;
		l2_power_table[i].power = l2_power;
	}

	if (i != num_opps) {
		ret = PTR_ERR(opp);
		goto free_l2_power_table;
	}

	cpufreq_device->cpu_dev = dev;
	cpufreq_device->dyn_power_table = power_table;
	cpufreq_device->dyn_l2_power_table = l2_power_table;
	cpufreq_device->dyn_power_table_entries = i;

	return ret;

free_l2_power_table:
	kfree(l2_power_table);

free_power_table:
	kfree(power_table);

	return ret;
}

static u32 cpu_freq_to_power(struct cpufreq_cooling_device *cpufreq_device,
			     u32 freq)
{
	int i;
	struct power_table *pt = cpufreq_device->dyn_power_table;

	for (i = 1; i < cpufreq_device->dyn_power_table_entries; i++)
		if (freq < pt[i].frequency)
			break;

	return pt[i - 1].power;
}

static u32 l2_freq_to_power(struct cpufreq_cooling_device *cpufreq_device,
				u32 freq)
{
	int i;
	struct power_table *pt = cpufreq_device->dyn_l2_power_table;

	for (i = 1; i < cpufreq_device->dyn_power_table_entries; i++)
		if (freq < pt[i].frequency)
			break;

	return pt[i - 1].power;
}

static u32 cpu_power_to_freq(struct cpufreq_cooling_device *cpufreq_device,
			     u32 power)
{
	int i;
	struct power_table *pt = cpufreq_device->dyn_power_table;

	for (i = 1; i < cpufreq_device->dyn_power_table_entries; i++)
		if (power < pt[i].power)
			break;

	return pt[i - 1].frequency;
}

/**
 * get_load() - get load for a cpu since last updated
 * @cpufreq_device:	&struct cpufreq_cooling_device for this cpu
 * @cpu:	cpu number
 * @cpu_idx:	index of the cpu in cpufreq_device->allowed_cpus
 *
 * Return: The average load of cpu @cpu in percentage since this
 * function was last called.
 */
static u32 get_load(struct cpufreq_cooling_device *cpufreq_device, int cpu,
		    int cpu_idx)
{
	u32 load;
	u64 now, now_idle, delta_time, delta_idle;

	now_idle = get_cpu_idle_time(cpu, &now, 0);
	delta_idle = now_idle - cpufreq_device->time_in_idle[cpu_idx];
	delta_time = now - cpufreq_device->time_in_idle_timestamp[cpu_idx];

	if (delta_time <= delta_idle)
		load = 0;
	else
		load = div64_u64(100 * (delta_time - delta_idle), delta_time);

	cpufreq_device->time_in_idle[cpu_idx] = now_idle;
	cpufreq_device->time_in_idle_timestamp[cpu_idx] = now;

	return load;
}

/**
 * get_static_power() - calculate the static power consumed by the cpus
 * @cpufreq_device:	struct &cpufreq_cooling_device for this cpu cdev
 * @tz:		thermal zone device in which we're operating
 * @freq:	frequency in KHz
 * @power:	pointer in which to store the calculated static power
 *
 * Calculate the static power consumed by the cpus described by
 * @cpufreq_device running at frequency @freq.  This function relies
 * on a platform specific function that should have been provided when
 * the cooling device was registered.  If it wasn't, the static power
 * is assumed to be negligible.  The calculated static power is stored
 * in @power.
 *
 * Return: 0 on success, -E* on failure.
 */
static int get_static_power(struct cpufreq_cooling_device *cpufreq_device,
			    struct thermal_zone_device *tz, unsigned long freq,
			    u32 *power)
{
	struct dev_pm_opp *opp;
	unsigned long voltage;
	struct cpumask our_online_cpus;
	unsigned long freq_hz = freq * 1000;
	int temperature;

#ifdef CONFIG_SPRD_CPU_COOLING_CPUIDLE
	cpumask_and(&our_online_cpus,
		&cpufreq_device->allowed_cpus, &cpufreq_device->active_cpus);
#else
	cpumask_and(&our_online_cpus,
		&cpufreq_device->allowed_cpus, cpu_online_mask);
#endif
	if (!cpufreq_device->plat_get_static_power ||
					!cpufreq_device->cpu_dev ||
					cpumask_empty(&our_online_cpus)) {
		*power = 0;
		return 0;
	}

	opp = dev_pm_opp_find_freq_exact(cpufreq_device->cpu_dev, freq_hz,
					 true);
	if (IS_ERR(opp)) {
		dev_warn_ratelimited(cpufreq_device->cpu_dev,
				"Failed to find OPP for frequency %lu: %ld\n",
				freq_hz, PTR_ERR(opp));
		return -EINVAL;
	}
	voltage = dev_pm_opp_get_voltage(opp);
	dev_pm_opp_put(opp);

	if (voltage == 0) {
		dev_warn_ratelimited(cpufreq_device->cpu_dev,
				     "Failed to get voltage for frequency %lu: %ld\n",
				     freq_hz, IS_ERR(opp) ? PTR_ERR(opp) : 0);
		return -EINVAL;
	}

	temperature = tz->temperature;

	return cpufreq_device->plat_get_static_power(
			&our_online_cpus, tz->passive_delay,
				voltage, power, temperature);
}

/**
 * get_dynamic_power() - calculate the dynamic power
 * @cpufreq_device:	&cpufreq_cooling_device for this cdev
 * @freq:	current frequency
 *
 * Return: the dynamic power consumed by the cpus described by
 * @cpufreq_device.
 */
static u32 get_dynamic_power(struct cpufreq_cooling_device *cpufreq_device,
			     unsigned long freq)
{
	u32 raw_cpu_power;

	raw_cpu_power = cpu_freq_to_power(cpufreq_device, freq);
	return (raw_cpu_power * cpufreq_device->last_load) / 100;
}

static u32 get_dynamic_l2_power(struct cpufreq_cooling_device *cpufreq_device,
				unsigned long freq)
{
	u32 raw_cpu_power, load;
	struct cpumask our_online_cpus;
	unsigned int num_our_online_cpus;

#ifdef CONFIG_SPRD_CPU_COOLING_CPUIDLE
	cpumask_and(&our_online_cpus,
		&cpufreq_device->allowed_cpus, &cpufreq_device->active_cpus);
#else
	cpumask_and(&our_online_cpus,
		&cpufreq_device->allowed_cpus, cpu_online_mask);
#endif
	num_our_online_cpus = cpumask_weight(&our_online_cpus);
	if (num_our_online_cpus == 0)
		load = 0;
	else
		load = cpufreq_device->last_load;
	raw_cpu_power = l2_freq_to_power(cpufreq_device, freq);
	return (raw_cpu_power * load) / 100;
}

/** get_cluster_core_num() - return cluster number of this cpu
 * @cpu: cpu id;
 * TODO: CONFIG_X86 is not accurate because the function is
 * work for iwhale2, We should fix topology_core_cpumask() to
 * get a right core_num rather than use a marco to distinguish it.
 */
static int get_cluster_core_num(int cpu)
{
	int core_num = -1;

#if defined(CONFIG_X86)
	core_num = 4;
#else
	core_num = cpumask_weight(topology_core_cpumask(cpu));
	if (core_num <= 0) {
		pr_err("hotplug: topology core cpumask error!\n");
		return -EINVAL;
	}
	pr_info("hotplug: test cpu:%u, cpu num in cluster: %u\n",
		cpu, core_num);
#endif

	return core_num;
}

/** get_cluster_id() - return cluster id of this cpu
 * @cpu: cpu id;
 * TODO: CONFIG_X86 is not accurate because the function is
 * work for iwhale2, We should fix topology_physical_package_id() to
 * get a right cluster id rather than use a marco to distinguish it.
 */
static int get_cluster_id(int cpu)
{
#if defined(CONFIG_X86)
	return ((cpu) < 4 ? 0 : 1);
#else
	return topology_physical_package_id((cpu));
#endif
}

static int init_pm_qos_request(struct cpufreq_cooling_device *cpufreq_dev)
{
	int cluster_id, pm_qos_class, init_cores;
	unsigned int first_cpu;

	first_cpu = cpumask_first(&cpufreq_dev->allowed_cpus);
	cluster_id = get_cluster_id(first_cpu);
	pm_qos_class = HOTPLUG_CLUSTER_NUM(cluster_id);
	init_cores = get_cluster_core_num(first_cpu);
	if (init_cores <= 0) {
		pr_err("get invaild cpu core number\n");
		return -EINVAL;
	}
	pm_qos_add_request(&cpufreq_dev->max_cpu_request,
		pm_qos_class, init_cores);
	cpufreq_dev->qos_cur_cpu = init_cores;

	pr_debug("first cpu : %u number of cpu :%u pm_qos_class:%d\n",
			first_cpu, cpufreq_dev->qos_cur_cpu, pm_qos_class);

	return 0;
}

#if defined(CONFIG_SPRD_CPU_COOLING_CPUIDLE) && defined(CONFIG_SPRD_CORE_CTL)
static void corectl_do_cpuidle(struct cpufreq_cooling_device *cpufreq_device)
{
	int cpu, min_temp;
	struct cpumask cpus;

	cpu = cpumask_any(&cpufreq_device->allowed_cpus);
	cpu = cpufreq_device->power_model->cab->get_min_temp_unisolated_core_p(
			cpufreq_device->power_model->cluster_id,
			cpu, &min_temp);

	if (cpu < 0)
		return;

	cpumask_clear(&cpus);
	if (cpu_online(cpu) && !cpu_isolated(cpu)) {

		pr_info("cpu%d min_temp:%d do cpuidle\n", cpu, min_temp);
		cpumask_set_cpu(cpu, &cpus);
		if (ctrl_core_api(&cpus, 0)) {

			cpumask_clear_cpu(cpu, &cpufreq_device->idle_cpus);
			return;
		}

		cpumask_set_cpu(cpu, &cpufreq_device->idle_cpus);
		cpumask_clear_cpu(cpu, &cpufreq_device->active_cpus);

	}

}

static void corectl_exit_cpuidle(struct cpufreq_cooling_device *cpufreq_device)
{
	int cpu, min_temp;
	struct cpumask cpus;

	cpu = cpumask_any(&cpufreq_device->allowed_cpus);
	cpu = cpufreq_device->power_model->cab->get_min_temp_isolated_core_p(
			cpufreq_device->power_model->cluster_id,
			cpu, &min_temp);

	if (cpu < 0)
		return;

	cpumask_clear(&cpus);
	if (cpu_online(cpu) && cpu_isolated(cpu)) {

		pr_info("cpu%d min_temp:%d exit cpuidle\n", cpu, min_temp);
		cpumask_set_cpu(cpu, &cpus);
		if (ctrl_core_api(&cpus, 1))
			return;

		cpumask_clear_cpu(cpu, &cpufreq_device->idle_cpus);
		cpumask_set_cpu(cpu, &cpufreq_device->active_cpus);
	}

}

static void corectl_allow_max_cpus(struct cpufreq_cooling_device *cpufreq_device)
{
	unsigned int cpu;
	struct cpumask cpus;

	cpumask_clear(&cpus);
	cpu = cpumask_any(&cpufreq_device->allowed_cpus);

	for_each_cpu(cpu, &cpufreq_device->allowed_cpus) {
		if (cpu_online(cpu) && cpu_isolated(cpu)) {
			cpumask_set_cpu(cpu, &cpus);

			cpumask_clear_cpu(cpu, &cpufreq_device->idle_cpus);
			cpumask_set_cpu(cpu, &cpufreq_device->active_cpus);

		}
	}

	cpu = cpumask_any(&cpufreq_device->allowed_cpus);
	pr_info("cpu%d all core exit cpuidle\n", cpu);
	ctrl_core_api(&cpus, 1);

}

static void corectl_adjust_cpus(struct cpufreq_cooling_device *cpufreq_device)
{
	int cpu;
	struct cpumask isolate_mask, allow_mask;
	u32 isolate_cpus, idle_cpus, active_cpus;

	cpu = cpumask_any(&cpufreq_device->allowed_cpus);
	cpumask_and(&isolate_mask, &cpufreq_device->allowed_cpus,
		    cpu_isolated_mask);
	isolate_cpus = cpumask_weight(&isolate_mask);
	idle_cpus = cpumask_weight(&cpufreq_device->idle_cpus);

	if (isolate_cpus != idle_cpus) {

		pr_info("cpu%d isolate_cpus:%u idle_cpus:%u adjust cpus!!!\n",
				cpu, isolate_cpus, idle_cpus);

		cpumask_andnot(&allow_mask, &cpufreq_device->allowed_cpus,
				cpu_isolated_mask);

		cpumask_copy(&cpufreq_device->active_cpus, &allow_mask);
		cpumask_copy(&cpufreq_device->idle_cpus, &isolate_mask);


		idle_cpus = cpumask_weight(&cpufreq_device->idle_cpus);
		active_cpus = cpumask_weight(&cpufreq_device->active_cpus);
		cpufreq_device->qos_cur_cpu = idle_cpus;

		pr_info("cpu%d active_cpus:%u idle_cpus:%u adjust ok!\n",
				cpu, active_cpus, idle_cpus);

	}
}

static void corectl_idle_func(struct work_struct *work)
{
	struct cpufreq_cooling_device *cpufreq_device = container_of(work,
			struct cpufreq_cooling_device, idle_work.work);

	corectl_do_cpuidle(cpufreq_device);
}
#endif

/*
 * We only change the online cpus after being called
 * hotplug_refractory_period times and then we average all the rounds.
 * This is to smooth the hotplug curve and prevent pingponging cpus
 */
static void set_online_cpus(struct cpufreq_cooling_device *cpufreq_dev,
			unsigned int target_online_cpus)
{
	int cpu;
	struct cpumask our_online_cpus;
	u32 current_online_cpus;
	u32 min_cpunum = 0, min_cpufreq = 0;
	struct online_data *online_data = &cpufreq_dev->online_data;

	online_data->target_online_cpus += target_online_cpus;
	online_data->rounds++;

	if (online_data->rounds < cpufreq_dev->hotplug_refractory_period)
		return;

	target_online_cpus = online_data->target_online_cpus
				/ online_data->rounds;
	online_data->target_online_cpus = 0;
	online_data->rounds = 0;

	cpu = cpumask_any(&cpufreq_dev->allowed_cpus);

#ifdef CONFIG_SPRD_CPU_COOLING_CPUIDLE
	cpumask_and(&our_online_cpus, &cpufreq_dev->allowed_cpus,
			&cpufreq_dev->active_cpus);
#else
	cpumask_and(&our_online_cpus, &cpufreq_dev->allowed_cpus,
		    cpu_online_mask);
#endif

	current_online_cpus = cpumask_weight(&our_online_cpus);

	if (cpufreq_dev->power_model->cab->
			get_cluster_min_cpufreq_p != NULL)
			min_cpufreq = cpufreq_dev->power_model->
				cab->get_cluster_min_cpufreq_p(
				cpufreq_dev->power_model->cluster_id);
	if (cpufreq_dev->power_model->cab->get_cluster_min_cpunum_p != NULL)
		min_cpunum = cpufreq_dev->power_model->cab->
			get_cluster_min_cpunum_p(
				cpufreq_dev->power_model->cluster_id);

	pr_debug("cpu%d curr_online:%d target_online:%d min_online:%d", cpu,
		current_online_cpus, target_online_cpus, min_cpunum);

	cpufreq_dev->qos_cur_cpu = max(target_online_cpus, min_cpunum);
	if (current_online_cpus != cpufreq_dev->qos_cur_cpu) {

		if ((current_online_cpus > cpufreq_dev->qos_cur_cpu) &&
				(cpufreq_dev->clipped_freq != min_cpufreq))
			return;

#if defined(CONFIG_SPRD_CPU_COOLING_CPUIDLE) && defined(CONFIG_SPRD_CORE_CTL)
		if (current_online_cpus >
				cpufreq_dev->qos_cur_cpu)
			schedule_delayed_work(&cpufreq_dev->idle_work, msecs_to_jiffies(80));
		else if (cpufreq_dev->qos_cur_cpu ==
				cpumask_weight(&cpufreq_dev->allowed_cpus))
			corectl_allow_max_cpus(cpufreq_dev);
		else
			corectl_exit_cpuidle(cpufreq_dev);
#else
		pm_qos_update_request(&cpufreq_dev->max_cpu_request,
			cpufreq_dev->qos_cur_cpu);

		pr_debug("cpu%d trigger hotplug... pm_qos_max_online:%d\n", cpu,
			cpufreq_dev->qos_cur_cpu);
#endif
	}
}

static int estimate_core_static_power(
				struct cpufreq_cooling_device *cpufreq_device,
				struct thermal_zone_device *tz, unsigned long freq,
				u32 *power, cpumask_t *cpumask)
{
	struct dev_pm_opp *opp;
	unsigned long voltage;
	struct cpumask our_online_cpus = *cpumask;
	unsigned long freq_hz = freq * 1000;
	int temperature;

	if (!cpufreq_device->plat_get_core_static_power ||
					!cpufreq_device->cpu_dev ||
					cpumask_empty(&our_online_cpus)) {
		*power = 0;
		return 0;
	}

	opp = dev_pm_opp_find_freq_exact(cpufreq_device->cpu_dev, freq_hz,
					 true);
	if (IS_ERR(opp)) {
		dev_warn_ratelimited(cpufreq_device->cpu_dev,
				"Failed to find OPP for frequency %lu: %ld\n",
				freq_hz, PTR_ERR(opp));
		return -EINVAL;
	}
	voltage = dev_pm_opp_get_voltage(opp);
	dev_pm_opp_put(opp);

	if (voltage == 0) {
		dev_warn_ratelimited(cpufreq_device->cpu_dev,
				     "Failed to get voltage for frequency %lu: %ld\n",
				     freq_hz, IS_ERR(opp) ? PTR_ERR(opp) : 0);
		return -EINVAL;
	}

	temperature = tz->temperature;

	return cpufreq_device->plat_get_core_static_power(
			&our_online_cpus, tz->passive_delay,
				voltage, power, temperature);
}

static void hotplug_out_cpus(struct cpufreq_cooling_device *cpufreq_device,
			u32 target_power, unsigned int target_freq)
{
	int i, cpu;
	u32 raw_cpu_power, estimated_power, per_cpu_load;
	unsigned int target_online_cpus, num_our_online_cpus;
	struct cpumask our_online_cpus;

	if (target_power) {

		cpu = cpumask_any(&cpufreq_device->allowed_cpus);
		raw_cpu_power = cpu_freq_to_power(cpufreq_device, target_freq);

#ifdef CONFIG_SPRD_CPU_COOLING_CPUIDLE
		cpumask_and(&our_online_cpus,
			&cpufreq_device->allowed_cpus, &cpufreq_device->active_cpus);
#else
		cpumask_and(&our_online_cpus,
			&cpufreq_device->allowed_cpus, cpu_online_mask);
#endif

		num_our_online_cpus = cpumask_weight(&our_online_cpus);
		if (!num_our_online_cpus)
			return;
		per_cpu_load = cpufreq_device->last_load / num_our_online_cpus;
		per_cpu_load = per_cpu_load ?: 1;

		estimated_power = 0;
		pr_debug("cpu%d hotplug_out raw:%u load:%u\n",
				cpu, raw_cpu_power, per_cpu_load);
		for (i = 0; i <= num_our_online_cpus; i++) {
			if (estimated_power > target_power)
				break;

			estimated_power += (raw_cpu_power * per_cpu_load) / 100;
			pr_debug("cpu%d hotplug_out cpus:%d est_power:%u\n",
					cpu, i, estimated_power);
		}

		target_online_cpus = max(i - 1, 0);

	} else
		target_online_cpus = 0;

	set_online_cpus(cpufreq_device, target_online_cpus);

}

static void hotplug_in_cpus(struct cpufreq_cooling_device *cpufreq_device,
			struct thermal_zone_device *tz, u32 target_power, unsigned int target_freq)
{
	int i, cpu;
	u32 raw_cpu_power, estimated_power, per_cpu_load;
	int target_online_cpus, allow_online_cpus, curr_online_cpus;
	struct cpumask our_online_cpus, clip_cpus;
	u32 static_power;

	raw_cpu_power = cpu_freq_to_power(cpufreq_device, target_freq);

#ifdef CONFIG_SPRD_CPU_COOLING_CPUIDLE
	cpumask_and(&our_online_cpus,
			&cpufreq_device->allowed_cpus, &cpufreq_device->active_cpus);
#else
	cpumask_and(&our_online_cpus,
		&cpufreq_device->allowed_cpus, cpu_online_mask);
#endif

	curr_online_cpus = cpumask_weight(&our_online_cpus);
	cpumask_clear(&clip_cpus);
	cpu = cpumask_any(&cpufreq_device->allowed_cpus);
	cpumask_set_cpu(cpu, &clip_cpus);
	estimate_core_static_power(cpufreq_device, tz, target_freq, &static_power, &clip_cpus);

	estimated_power = 0;
	allow_online_cpus = cpumask_weight(&cpufreq_device->allowed_cpus);
	if (allow_online_cpus != 0)
		per_cpu_load = cpufreq_device->last_load / allow_online_cpus;
	else
		per_cpu_load = cpufreq_device->last_load;

	per_cpu_load = per_cpu_load ?: 1;
	pr_debug("cpu%d hotplug_in raw:%u sta:%u tar:%u load:%u\n",
			cpu, raw_cpu_power, static_power, target_power, per_cpu_load);
	for (i = 0; i <= allow_online_cpus; i++) {
		estimated_power += (raw_cpu_power * per_cpu_load) / 100;
		if (i >= curr_online_cpus)
			estimated_power += static_power;

		pr_debug("cpu%d hotplug_in cpus:%d est_power:%u\n",
				cpu, i, estimated_power);
		if (estimated_power > target_power)
			break;
	}

	target_online_cpus = min(i, allow_online_cpus);
	set_online_cpus(cpufreq_device, target_online_cpus);

}

static void hotplug_keep_cpus(struct cpufreq_cooling_device *cpufreq_device)
{
	struct cpumask current_online_cpus;

#ifdef CONFIG_SPRD_CPU_COOLING_CPUIDLE
	cpumask_and(&current_online_cpus,
			&cpufreq_device->allowed_cpus, &cpufreq_device->active_cpus);
#else
	cpumask_and(&current_online_cpus,
		&cpufreq_device->allowed_cpus, cpu_online_mask);
#endif

	set_online_cpus(cpufreq_device, cpumask_weight(&current_online_cpus));
}

/* cpufreq cooling device callback functions are defined below */

/**
 * cpufreq_get_max_state - callback function to get the max cooling state.
 * @cdev: thermal cooling device pointer.
 * @state: fill this variable with the max cooling state.
 *
 * Callback for the thermal cooling device to return the cpufreq
 * max cooling state.
 *
 * Return: 0 on success, an error code otherwise.
 */
static int cpufreq_get_max_state(struct thermal_cooling_device *cdev,
				 unsigned long *state)
{
	struct cpufreq_cooling_device *cpufreq_device = cdev->devdata;

	*state = cpufreq_device->max_level;
	return 0;
}

/**
 * cpufreq_get_cur_state - callback function to get the current cooling state.
 * @cdev: thermal cooling device pointer.
 * @state: fill this variable with the current cooling state.
 *
 * Callback for the thermal cooling device to return the cpufreq
 * current cooling state.
 *
 * Return: 0 on success, an error code otherwise.
 */
static int cpufreq_get_cur_state(struct thermal_cooling_device *cdev,
				 unsigned long *state)
{
	struct cpufreq_cooling_device *cpufreq_device = cdev->devdata;

	*state = cpufreq_device->cpufreq_state;

	return 0;
}

/**
 * cpufreq_set_cur_state - callback function to set the current cooling state.
 * @cdev: thermal cooling device pointer.
 * @state: set this variable to the current cooling state.
 *
 * Callback for the thermal cooling device to change the cpufreq
 * current cooling state.
 *
 * Return: 0 on success, an error code otherwise.
 */
static int cpufreq_set_cur_state(struct thermal_cooling_device *cdev,
				 unsigned long state)
{
	struct cpufreq_cooling_device *cpufreq_device = cdev->devdata;
	unsigned int cpu = cpumask_any(&cpufreq_device->allowed_cpus);
	unsigned int clip_freq;

	/* Request state should be less than max_level */
	if (WARN_ON(state > cpufreq_device->max_level))
		return -EINVAL;

	/* Check if the old cooling action is same as new cooling action */
	if ((cpufreq_device->cpufreq_state == state) &&
		(cpufreq_device->clipped_freq ==
		cpufreq_device->freq_table[state]))
		return 0;

	clip_freq = cpufreq_device->freq_table[state];
	cpufreq_device->cpufreq_state = state;
	cpufreq_device->clipped_freq = clip_freq;

	cpufreq_update_policy(cpu);

	pr_info("cpu%u update max_freq to %u\n", cpu, clip_freq);

	return 0;
}

/**
 * cpufreq_get_requested_power() - get the current power
 * @cdev:	&thermal_cooling_device pointer
 * @tz:		a valid thermal zone device pointer
 * @power:	pointer in which to store the resulting power
 *
 * Calculate the current power consumption of the cpus in milliwatts
 * and store it in @power.  This function should actually calculate
 * the requested power, but it's hard to get the frequency that
 * cpufreq would have assigned if there were no thermal limits.
 * Instead, we calculate the current power on the assumption that the
 * immediate future will look like the immediate past.
 *
 * We use the current frequency and the average load since this
 * function was last called.  In reality, there could have been
 * multiple opps since this function was last called and that affects
 * the load calculation.  While it's not perfectly accurate, this
 * simplification is good enough and works.  REVISIT this, as more
 * complex code may be needed if experiments show that it's not
 * accurate enough.
 *
 * Return: 0 on success, -E* if getting the static power failed.
 */
static int cpufreq_get_requested_power(struct thermal_cooling_device *cdev,
				       struct thermal_zone_device *tz,
				       u32 *power)
{
	unsigned long freq;
	int i = 0, cpu, ret;
	u32 static_power, dynamic_power, total_load = 0;
	struct cpufreq_cooling_device *cpufreq_device = cdev->devdata;
	u32 *load_cpu = NULL;

	get_online_cpus();
	cpu = cpumask_any(&cpufreq_device->allowed_cpus);

	/*
	 * All the CPUs are offline, thus the requested power by
	 * the cdev is 0
	 */
	if (cpu >= nr_cpu_ids) {
		*power = 0;
		put_online_cpus();
		return 0;
	}

	freq = cpufreq_quick_get_target(cpu);
	put_online_cpus();

	if (trace_thermal_power_cpu_get_power_enabled()) {
		u32 ncpus = cpumask_weight(&cpufreq_device->allowed_cpus);

		load_cpu = kcalloc(ncpus, sizeof(*load_cpu), GFP_KERNEL);
	}

	for_each_cpu(cpu, &cpufreq_device->allowed_cpus) {
		u32 load;

		if (cpu_online(cpu))
			load = get_load(cpufreq_device, cpu, i);
		else
			load = 0;

		total_load += load;
		if (trace_thermal_power_cpu_limit_enabled() && load_cpu)
			load_cpu[i] = load;

		i++;
	}

	freq = freq ?: cpufreq_device->freq_table[0];
	total_load = total_load ?: 100;
	cpufreq_device->last_load = total_load;

#if defined(CONFIG_SPRD_CPU_COOLING_CPUIDLE) && defined(CONFIG_SPRD_CORE_CTL)
	corectl_adjust_cpus(cpufreq_device);
#endif

	dynamic_power = get_dynamic_power(cpufreq_device, freq);
	dynamic_power += get_dynamic_l2_power(cpufreq_device, freq);

	ret = get_static_power(cpufreq_device, tz, freq, &static_power);
	if (ret) {
		kfree(load_cpu);
		return ret;
	}

	if (load_cpu) {
		trace_thermal_power_cpu_get_power(
			&cpufreq_device->allowed_cpus,
			freq, load_cpu, i, dynamic_power, static_power);

		kfree(load_cpu);
	}

	cpu = cpumask_any(&cpufreq_device->allowed_cpus);
	*power = static_power + dynamic_power;
	pr_debug("cpu%d freq:%lu static:%u dynamic:%u load:%u\n",
		cpu, freq, static_power, dynamic_power,
		cpufreq_device->last_load);
	return 0;
}

/**
 * cpufreq_state2power() - convert a cpu cdev state to power consumed
 * @cdev:	&thermal_cooling_device pointer
 * @tz:		a valid thermal zone device pointer
 * @state:	cooling device state to be converted
 * @power:	pointer in which to store the resulting power
 *
 * Convert cooling device state @state into power consumption in
 * milliwatts assuming 100% load.  Store the calculated power in
 * @power.
 *
 * Return: 0 on success, -EINVAL if the cooling device state could not
 * be converted into a frequency or other -E* if there was an error
 * when calculating the static power.
 */
static int cpufreq_state2power(struct thermal_cooling_device *cdev,
			       struct thermal_zone_device *tz,
			       unsigned long state, u32 *power)
{
	unsigned int freq, num_cpus;
	u32 static_power, dynamic_power;
	int ret;
	struct cpufreq_cooling_device *cpufreq_device = cdev->devdata;

	num_cpus = cpumask_weight(&cpufreq_device->allowed_cpus);

	freq = cpufreq_device->freq_table[state];
	if (!freq)
		return -EINVAL;

	dynamic_power = cpu_freq_to_power(cpufreq_device, freq) * num_cpus;
	dynamic_power += l2_freq_to_power(cpufreq_device, freq) * num_cpus;
	ret = get_static_power(cpufreq_device, tz, freq, &static_power);
	if (ret)
		return ret;

	*power = static_power + dynamic_power;
	return 0;
}

/**
 * cpufreq_power2state() - convert power to a cooling device state
 * @cdev:	&thermal_cooling_device pointer
 * @tz:		a valid thermal zone device pointer
 * @power:	power in milliwatts to be converted
 * @state:	pointer in which to store the resulting state
 *
 * Calculate a cooling device state for the cpus described by @cdev
 * that would allow them to consume at most @power mW and store it in
 * @state.  Note that this calculation depends on external factors
 * such as the cpu load or the current static power.  Calling this
 * function with the same power as input can yield different cooling
 * device states depending on those external factors.
 *
 * Return: 0 on success, -ENODEV if no cpus are online or -EINVAL if
 * the calculated frequency could not be converted to a valid state.
 * The latter should not happen unless the frequencies available to
 * cpufreq have changed since the initialization of the cpu cooling
 * device.
 */
static int cpufreq_power2state(struct thermal_cooling_device *cdev,
			       struct thermal_zone_device *tz, u32 power,
			       unsigned long *state)
{
	unsigned int cpu, target_freq, num_our_online_cpus;
	int ret;
	u32 last_load, normalised_power;
	struct cpufreq_cooling_device *cpufreq_device = cdev->devdata;
	struct cpumask our_online_cpus;
	unsigned int min_cpufreq = 0;
	static int last_temperature;
	static int count;
	s32 dyn_power;

	get_online_cpus();
	cpu = cpumask_any(&cpufreq_device->allowed_cpus);
	last_load = cpufreq_device->last_load ?: 100;

	/* None of our cpus are online */
	if (cpu < nr_cpu_ids) {
		unsigned int cur_freq;
		u32 static_power;
		u32 l2_power;

		cur_freq = cpufreq_quick_get_target(cpu);
		put_online_cpus();
		ret = get_static_power(cpufreq_device,
					tz, cur_freq, &static_power);
		if (ret)
			return ret;

		l2_power = get_dynamic_l2_power(cpufreq_device, cur_freq);

		if (power > (static_power+l2_power))
			dyn_power = power - static_power - l2_power;
		else
			dyn_power  = 0;

		normalised_power = (dyn_power * 100) / last_load;
		target_freq = cpu_power_to_freq(cpufreq_device,
						normalised_power);

		if (cpufreq_device->power_model->cab->
			get_cluster_min_cpufreq_p != NULL)
			min_cpufreq = cpufreq_device->power_model->
				cab->get_cluster_min_cpufreq_p(
				cpufreq_device->power_model->cluster_id);
		if (min_cpufreq)
			if (target_freq < min_cpufreq)
				target_freq = min_cpufreq;

		*state = cpufreq_cooling_get_level(cpu, target_freq);
		if (*state == THERMAL_CSTATE_INVALID) {
			dev_warn_ratelimited(&cdev->device,
			"Failed to convert %dKHz for cpu %d into a cdev state\n",
					     target_freq, cpu);
			return -EINVAL;
		}

		pr_debug("cpu%d dyn:%u sta:%u l2:%u allow:%u\n",
				cpu, dyn_power, static_power, l2_power, power);
	} else {
		put_online_cpus();
		dyn_power = power;
		normalised_power = (dyn_power * 100) / last_load;
		target_freq = cpu_power_to_freq(cpufreq_device,
						normalised_power);
		if (cpufreq_device->power_model->cab->
			get_cluster_min_cpufreq_p != NULL)
			min_cpufreq = cpufreq_device->power_model->
				cab->get_cluster_min_cpufreq_p(
				cpufreq_device->power_model->cluster_id);
		if (min_cpufreq)
			target_freq = max(min_cpufreq, target_freq);

		cpu = cpumask_any(&cpufreq_device->allowed_cpus);
		*state = cpufreq_cooling_get_level(cpu, target_freq);
		if (*state == THERMAL_CSTATE_INVALID) {
			dev_warn_ratelimited(&cdev->device,
			"Failed to convert %dKHz for cpu %d into a cdev state\n",
					     target_freq, cpu);
			return -EINVAL;
		}
	}

#ifdef CONFIG_SPRD_CPU_COOLING_CPUIDLE
	cpumask_and(&our_online_cpus, &cpufreq_device->allowed_cpus,
			&cpufreq_device->active_cpus);
#else
	cpumask_and(&our_online_cpus,
		&cpufreq_device->allowed_cpus, cpu_online_mask);
#endif

	num_our_online_cpus = cpumask_weight(&our_online_cpus);
	if (cpufreq_device->hotplug_refractory_period) {
		if (normalised_power <
			cpu_freq_to_power(cpufreq_device, target_freq))
			hotplug_out_cpus(cpufreq_device,
				dyn_power, target_freq);
		else if (num_our_online_cpus <
			cpumask_weight(&cpufreq_device->allowed_cpus))
			hotplug_in_cpus(cpufreq_device, tz,
				dyn_power, target_freq);
		else
			hotplug_keep_cpus(cpufreq_device);
	}
	trace_thermal_power_cpu_limit(&cpufreq_device->allowed_cpus,
				      target_freq, *state, power);

#ifdef CONFIG_SPRD_CPU_COOLING_CPUIDLE
	cpumask_and(&our_online_cpus, &cpufreq_device->allowed_cpus,
			&cpufreq_device->active_cpus);
#else
	cpumask_and(&our_online_cpus,
		&cpufreq_device->allowed_cpus, cpu_online_mask);
#endif

	num_our_online_cpus = cpumask_weight(&our_online_cpus);
	cpu = cpumask_any(&cpufreq_device->allowed_cpus);
	if (count == 49) {
		if (tz->temperature != last_temperature)
		pr_info("cpu%u temp:%u target_freq:%u online:%u qos_cpu:%u power:%u\n",
			cpu, tz->temperature, target_freq, num_our_online_cpus,
			cpufreq_device->qos_cur_cpu, power);
		last_temperature = tz->temperature;
		count = 0;
	}
	count++;

	return 0;
}

static int cpufreq_online_everything(struct thermal_cooling_device *cdev)
{
	struct cpufreq_cooling_device *cpufreq_device = cdev->devdata;
	unsigned int num_cpus = cpumask_weight(&cpufreq_device->allowed_cpus);

	if (cpufreq_device->hotplug_refractory_period) {
		cpufreq_device->online_data.rounds =
			cpufreq_device->hotplug_refractory_period;
		cpufreq_device->online_data.target_online_cpus = num_cpus *
			cpufreq_device->hotplug_refractory_period;
		set_online_cpus(cpufreq_device, num_cpus);
	}

	return 0;
}

static unsigned int find_next_max(struct cpufreq_frequency_table *table,
				  unsigned int prev_max)
{
	struct cpufreq_frequency_table *pos;
	unsigned int max = 0;

	cpufreq_for_each_valid_entry(pos, table) {
		if (pos->frequency > max && pos->frequency < prev_max)
			max = pos->frequency;
	}

	return max;
}

static int update_dyn_power_table(struct cpufreq_cooling_device *cpufreq_device,
				 int cluster)
{
	struct power_table *power_table = cpufreq_device->dyn_power_table;
	struct power_table *l2_power_table = cpufreq_device->dyn_l2_power_table;
	struct dev_pm_opp *opp = NULL;
	struct device *dev = NULL;
	int num_opps = 0, cpu, i, ret = 0, count = 0;
	unsigned long freq;

	for_each_cpu(cpu, &cpufreq_device->allowed_cpus) {
		dev = get_cpu_device(cpu);
		if (!dev) {
			dev_warn(&cpufreq_device->cool_dev->device,
				 "No cpu device for cpu %d\n", cpu);
			continue;
		}

		num_opps = dev_pm_opp_get_opp_count(dev);
		if (num_opps > 0)
			break;
		else if (num_opps < 0)
			return num_opps;
	}

	if (num_opps == 0)
		return -EINVAL;

	memset(power_table, 0, sizeof(struct power_table)*num_opps);
	memset(l2_power_table, 0, sizeof(struct power_table)*num_opps);

	for (freq = 0, i = 0;
	     opp = dev_pm_opp_find_freq_ceil(dev, &freq), !IS_ERR(opp);
	     freq++, i++) {
		u32 freq_mhz, voltage_mv;
		u32 power = 0, l2_power = 0;

		if (i >= num_opps) {
			ret = -EAGAIN;
		}

		freq_mhz = freq / 1000000;
		voltage_mv = dev_pm_opp_get_voltage(opp) / 1000;
		dev_pm_opp_put(opp);

		/*
		 * Do the multiplication with MHz and millivolt so as
		 * to not overflow.
		 */
		if (cpufreq_device->power_model->
			cab->get_core_dyn_power_p != NULL)
			power = cpufreq_device->power_model->cab->
				get_core_dyn_power_p(cluster,
						freq_mhz, voltage_mv);

		if (cpufreq_device->power_model->
				cab->get_cluster_dyn_power_p != NULL)
			l2_power = cpufreq_device->power_model->
				cab->get_cluster_dyn_power_p(
					cluster, freq_mhz, voltage_mv);

		if ((freq / 1000) > cpufreq_device->curr_max_freq) {
			count++;
			continue;
		}

		/* frequency is stored in power_table in KHz */
		power_table[i].frequency = freq / 1000;
		l2_power_table[i].frequency = freq / 1000;

		/* power is stored in mW */
		power_table[i].power = power;
		l2_power_table[i].power = l2_power;

		pr_debug("%d.freq=%u vol=%u core_power=%u l2_power=%u\n",
			i, freq_mhz, voltage_mv, power, l2_power);
	}

	cpufreq_device->dyn_power_table_entries = i - count;

	return ret;
}
static int update_dyn_freq_table(struct cpufreq_cooling_device *cpufreq_device)
{
	unsigned int i, freq, count = 0;
	struct cpufreq_frequency_table *pos, *table;
	unsigned int cpu = cpumask_any(&cpufreq_device->allowed_cpus);
	int cluster_id = get_cluster_id(cpu);


	table = cpufreq_frequency_get_table(cpu);
	if (!table) {
		pr_debug("%s: CPUFreq table not found\n", __func__);
		return -ENODEV;
	}

	cpufreq_device->max_level = 0;
	/* Find max levels */
	cpufreq_for_each_valid_entry(pos, table)
		cpufreq_device->max_level++;


	memset(cpufreq_device->freq_table, 0,
		sizeof(*cpufreq_device->freq_table)*cpufreq_device->max_level);

	/* max_level is an index, not a counter */
	cpufreq_device->max_level--;

	/* Fill freq-table in descending order of frequencies */
	for (i = 0, freq = -1; i <= cpufreq_device->max_level; i++) {
		freq = find_next_max(table, freq);
		if (freq > cpufreq_device->curr_max_freq) {
			count++;
			continue;
		}
		cpufreq_device->freq_table[i-count] = freq;

		/* Warn for duplicate entries */
		if (!freq)
			pr_warn("%s: table has duplicate entries\n", __func__);
		else
			pr_debug("%s: freq:%u KHz level:%d\n", __func__,
				freq, i-count);

	}


	update_dyn_power_table(cpufreq_device, cluster_id);

	return 0;

}

static int cpufreq_update_max_freq(struct thermal_cooling_device *cdev,
					struct thermal_zone_device *tz)
{
	unsigned int allow_max_freq = 0;
	struct cpufreq_cooling_device *cpufreq_device = cdev->devdata;
	unsigned int cpu = cpumask_any(&cpufreq_device->allowed_cpus);

#if defined(CONFIG_ARM_SPRD_HW_CPUFREQ) || defined(CONFIG_ARM_SPRD_SW_CPUFREQ)
	allow_max_freq = sprd_cpufreq_update_opp(cpu, tz->temperature);
#endif

	pr_debug("cpu%u allow_max_freq:%u curr_max_freq:%u\n", cpu,
				allow_max_freq, cpufreq_device->curr_max_freq);
	if (allow_max_freq) {
		cpufreq_device->curr_max_freq = allow_max_freq;
		update_dyn_freq_table(cpufreq_device);

		cpufreq_set_cur_state(cdev, 0);

		pr_info("cpu%u allow max_freq to %u\n",
			cpu, cpufreq_device->curr_max_freq);
	}

	return 0;
}
/* Bind cpufreq callbacks to thermal cooling device ops */
static struct thermal_cooling_device_ops cpufreq_cooling_ops = {
	.get_max_state = cpufreq_get_max_state,
	.get_cur_state = cpufreq_get_cur_state,
	.set_cur_state = cpufreq_set_cur_state,
	.online_everything = cpufreq_online_everything,
	.update_max_freq = cpufreq_update_max_freq,
};

/* Notifier for cpufreq policy change */
static struct notifier_block thermal_cpufreq_notifier_block = {
	.notifier_call = cpufreq_thermal_notifier,
};

/**
 * __cpufreq_cooling_register - helper function to create cpufreq cooling device
 * @np: a valid struct device_node to the cooling device device tree node
 * @clip_cpus: cpumask of cpus where the frequency constraints will happen.
 * Normally this should be same as cpufreq policy->related_cpus.
 * @hotplug_period: periods to wait before hotplugging CPUs
 * @power_model: function to calculate the static power consumed by these
 *                    cpus (optional)
 *
 * This interface function registers the cpufreq cooling device with the name
 * "thermal-cpufreq-%x". This api can support multiple instances of cpufreq
 * cooling devices. It also gives the opportunity to link the cooling device
 * with a device tree node, in order to bind it via the thermal DT code.
 *
 * Return: a valid struct thermal_cooling_device pointer on success,
 * on failure, it returns a corresponding ERR_PTR().
 */
static struct thermal_cooling_device *
__cpufreq_cooling_register(struct device_node *np,
	const struct cpumask *clip_cpus, u32 hotplug_period,
		struct cpu_power_model_t *power_model)
{
	struct thermal_cooling_device *cool_dev;
	struct cpufreq_cooling_device *cpufreq_dev;
	char dev_name[THERMAL_NAME_LENGTH];
	struct cpufreq_frequency_table *pos, *table;
	unsigned int freq, i, num_cpus;
	int ret;

	table = cpufreq_frequency_get_table(cpumask_first(clip_cpus));
	if (!table) {
		pr_debug("%s: CPUFreq table not found\n", __func__);
		return ERR_PTR(-EPROBE_DEFER);
	}

	cpufreq_dev = kzalloc(sizeof(*cpufreq_dev), GFP_KERNEL);
	if (!cpufreq_dev)
		return ERR_PTR(-ENOMEM);

	num_cpus = cpumask_weight(clip_cpus);
	cpufreq_dev->time_in_idle = kcalloc(num_cpus,
					    sizeof(*cpufreq_dev->time_in_idle),
					    GFP_KERNEL);
	if (!cpufreq_dev->time_in_idle) {
		cool_dev = ERR_PTR(-ENOMEM);
		goto free_cdev;
	}

	cpufreq_dev->time_in_idle_timestamp =
		kcalloc(num_cpus, sizeof(*cpufreq_dev->time_in_idle_timestamp),
			GFP_KERNEL);
	if (!cpufreq_dev->time_in_idle_timestamp) {
		cool_dev = ERR_PTR(-ENOMEM);
		goto free_time_in_idle;
	}

	/* Find max levels */
	cpufreq_for_each_valid_entry(pos, table)
		cpufreq_dev->max_level++;

	cpufreq_dev->freq_table = kmalloc(sizeof(*cpufreq_dev->freq_table) *
					  cpufreq_dev->max_level, GFP_KERNEL);
	if (!cpufreq_dev->freq_table) {
		cool_dev = ERR_PTR(-ENOMEM);
		goto free_time_in_idle_timestamp;
	}

	/* max_level is an index, not a counter */
	cpufreq_dev->max_level--;

	cpumask_copy(&cpufreq_dev->allowed_cpus, clip_cpus);

#ifdef CONFIG_SPRD_CPU_COOLING_CPUIDLE
	cpumask_copy(&cpufreq_dev->active_cpus, clip_cpus);
	INIT_DELAYED_WORK(&cpufreq_dev->idle_work,
			corectl_idle_func);
#endif

	if (power_model != NULL) {
		cpufreq_cooling_ops.get_requested_power =
			cpufreq_get_requested_power;
		cpufreq_cooling_ops.state2power = cpufreq_state2power;
		cpufreq_cooling_ops.power2state = cpufreq_power2state;
		cpufreq_dev->plat_get_static_power =
			power_model->cab->get_static_power_p;
		cpufreq_dev->plat_get_core_static_power =
			power_model->cab->get_core_static_power_p;
		cpufreq_dev->power_model = power_model;
		ret = build_dyn_power_table(cpufreq_dev,
				power_model->cluster_id);
		if (ret) {
			cool_dev = ERR_PTR(ret);
			goto free_table;
		}
	}

	ret = get_idr(&cpufreq_idr, &cpufreq_dev->id);
	if (ret) {
		cool_dev = ERR_PTR(ret);
		goto free_power_table;
	}

	/* Fill freq-table in descending order of frequencies */
	for (i = 0, freq = -1; i <= cpufreq_dev->max_level; i++) {
		freq = find_next_max(table, freq);
		cpufreq_dev->freq_table[i] = freq;

		/* Warn for duplicate entries */
		if (!freq)
			pr_warn("%s: table has duplicate entries\n", __func__);
		else
			pr_debug("%s: freq:%u KHz\n", __func__, freq);
	}

	snprintf(dev_name, sizeof(dev_name), "thermal-cpufreq-%d",
		 cpufreq_dev->id);

	cool_dev = thermal_of_cooling_device_register(np, dev_name, cpufreq_dev,
						      &cpufreq_cooling_ops);
	if (IS_ERR(cool_dev))
		goto remove_idr;

	cpufreq_dev->clipped_freq = cpufreq_dev->freq_table[0];
	cpufreq_dev->curr_max_freq = cpufreq_dev->clipped_freq;
	cpufreq_dev->cool_dev = cool_dev;
	ret = init_pm_qos_request(cpufreq_dev);
	if (!ret)
		cpufreq_dev->hotplug_refractory_period = hotplug_period;
	else
		cpufreq_dev->hotplug_refractory_period = false;

	mutex_lock(&cooling_cpufreq_lock);

	mutex_lock(&cooling_list_lock);
	list_add(&cpufreq_dev->node, &cpufreq_dev_list);
	mutex_unlock(&cooling_list_lock);

	/* Register the notifier for first cpufreq cooling device */
	if (!cpufreq_dev_count++) {
		cpufreq_register_notifier(&thermal_cpufreq_notifier_block,
					  CPUFREQ_POLICY_NOTIFIER);

	}
	mutex_unlock(&cooling_cpufreq_lock);

	return cool_dev;

remove_idr:
	release_idr(&cpufreq_idr, cpufreq_dev->id);
free_power_table:
	kfree(cpufreq_dev->dyn_power_table);
	kfree(cpufreq_dev->dyn_l2_power_table);
free_table:
	kfree(cpufreq_dev->freq_table);
free_time_in_idle_timestamp:
	kfree(cpufreq_dev->time_in_idle_timestamp);
free_time_in_idle:
	kfree(cpufreq_dev->time_in_idle);
free_cdev:
	kfree(cpufreq_dev);

	return cool_dev;
}

/**
 * cpufreq_cooling_register - function to create cpufreq cooling device.
 * @clip_cpus: cpumask of cpus where the frequency constraints will happen.
 *
 * This interface function registers the cpufreq cooling device with the name
 * "thermal-cpufreq-%x". This api can support multiple instances of cpufreq
 * cooling devices.
 *
 * Return: a valid struct thermal_cooling_device pointer on success,
 * on failure, it returns a corresponding ERR_PTR().
 */
struct thermal_cooling_device *
cpufreq_cooling_register(const struct cpumask *clip_cpus)
{
	return __cpufreq_cooling_register(NULL, clip_cpus, false, NULL);
}
EXPORT_SYMBOL_GPL(cpufreq_cooling_register);

/**
 * of_cpufreq_cooling_register - function to create cpufreq cooling device.
 * @np: a valid struct device_node to the cooling device device tree node
 * @clip_cpus: cpumask of cpus where the frequency constraints will happen.
 *
 * This interface function registers the cpufreq cooling device with the name
 * "thermal-cpufreq-%x". This api can support multiple instances of cpufreq
 * cooling devices. Using this API, the cpufreq cooling device will be
 * linked to the device tree node provided.
 *
 * Return: a valid struct thermal_cooling_device pointer on success,
 * on failure, it returns a corresponding ERR_PTR().
 */
struct thermal_cooling_device *
of_cpufreq_cooling_register(struct device_node *np,
			    const struct cpumask *clip_cpus)
{
	if (!np)
		return ERR_PTR(-EINVAL);

	return __cpufreq_cooling_register(np, clip_cpus, false, NULL);
}
EXPORT_SYMBOL_GPL(of_cpufreq_cooling_register);

/**
 * cpufreq_power_cooling_register() - create cpufreq cooling device with power extensions
 * @clip_cpus:	cpumask of cpus where the frequency constraints will happen
 * @hotplug_period: periods to wait before hotplugging CPUs
 * @power_model:	function to calculate the static power consumed by these
 *			cpus (optional)
 *
 * This interface function registers the cpufreq cooling device with
 * the name "thermal-cpufreq-%x".  This api can support multiple
 * instances of cpufreq cooling devices.  Using this function, the
 * cooling device will implement the power extensions by using a
 * simple cpu power model.  The cpus must have registered their OPPs
 * using the OPP library.
 *
 * An optional @power_model may be provided to calculate the
 * static power consumed by these cpus.  If the platform's static
 * power consumption is unknown or negligible, make it NULL.
 *
 * Return: a valid struct thermal_cooling_device pointer on success,
 * on failure, it returns a corresponding ERR_PTR().
 */
struct thermal_cooling_device *
cpufreq_power_cooling_register(const struct cpumask *clip_cpus,
			       u32 hotplug_period,
			       struct cpu_power_model_t *power_model)
{
	return __cpufreq_cooling_register(NULL, clip_cpus,
				hotplug_period, power_model);
}
EXPORT_SYMBOL(cpufreq_power_cooling_register);

/**
 * of_cpufreq_power_cooling_register() - create cpufreq cooling device with power extensions
 * @np:	a valid struct device_node to the cooling device device tree node
 * @clip_cpus:	cpumask of cpus where the frequency constraints will happen
 * @hotplug_period: periods to wait before hotplugging CPUs
 * @power_model:	function to calculate the static power consumed by these
 *			cpus (optional)
 *
 * This interface function registers the cpufreq cooling device with
 * the name "thermal-cpufreq-%x".  This api can support multiple
 * instances of cpufreq cooling devices.  Using this API, the cpufreq
 * cooling device will be linked to the device tree node provided.
 * Using this function, the cooling device will implement the power
 * extensions by using a simple cpu power model.  The cpus must have
 * registered their OPPs using the OPP library.
 *
 * An optional @power_model may be provided to calculate the
 * static power consumed by these cpus.  If the platform's static
 * power consumption is unknown or negligible, make it NULL.
 *
 * Return: a valid struct thermal_cooling_device pointer on success,
 * on failure, it returns a corresponding ERR_PTR().
 */
struct thermal_cooling_device *
of_cpufreq_power_cooling_register(struct device_node *np,
		const struct cpumask *clip_cpus, u32 hotplug_period,
		struct cpu_power_model_t *power_model)
{
	if (!np)
		return ERR_PTR(-EINVAL);

	return __cpufreq_cooling_register(np, clip_cpus, hotplug_period,
					  power_model);
}
EXPORT_SYMBOL(of_cpufreq_power_cooling_register);

/**
 * cpufreq_cooling_unregister - function to remove cpufreq cooling device.
 * @cdev: thermal cooling device pointer.
 *
 * This interface function unregisters the "thermal-cpufreq-%x" cooling device.
 */
void cpufreq_cooling_unregister(struct thermal_cooling_device *cdev)
{
	struct cpufreq_cooling_device *cpufreq_dev;

	if (!cdev)
		return;

	cpufreq_dev = cdev->devdata;

#ifdef CONFIG_SPRD_CPU_COOLING_CPUIDLE
	cancel_delayed_work_sync(&cpufreq_dev->idle_work);
#endif

	/* Unregister the notifier for the last cpufreq cooling device */
	mutex_lock(&cooling_cpufreq_lock);
	if (!--cpufreq_dev_count) {
		cpufreq_unregister_notifier(&thermal_cpufreq_notifier_block,
					    CPUFREQ_POLICY_NOTIFIER);

	}
	mutex_lock(&cooling_list_lock);
	list_del(&cpufreq_dev->node);
	mutex_unlock(&cooling_list_lock);

	mutex_unlock(&cooling_cpufreq_lock);

	thermal_cooling_device_unregister(cpufreq_dev->cool_dev);
	release_idr(&cpufreq_idr, cpufreq_dev->id);
	kfree(cpufreq_dev->dyn_power_table);
	kfree(cpufreq_dev->dyn_l2_power_table);
	kfree(cpufreq_dev->time_in_idle_timestamp);
	kfree(cpufreq_dev->time_in_idle);
	kfree(cpufreq_dev->freq_table);
	kfree(cpufreq_dev);
}
EXPORT_SYMBOL_GPL(cpufreq_cooling_unregister);
